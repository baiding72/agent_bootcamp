import os
import json
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_classic.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.tools.tavily_search import TavilySearchResults

from models import UserProfile
from memory.profile_manager import UserProfileManager
from prompts.persona import get_chat_prompt, get_extraction_components

load_dotenv()

# --- 1. åˆå§‹åŒ– ---
llm = ChatOpenAI(model="qwen-plus", 
                 openai_api_key=os.getenv("QWEN_API_KEY"),
                 openai_api_base=os.getenv("QWEN_BASE_URL"),
                 temperature=0) # æå–ä¿¡æ¯æ—¶æ¸©åº¦è¦ä½
tools = [TavilySearchResults(max_results=1)]
profile_manager = UserProfileManager()

# --- 2. æ„å»º Chat Agent (Runnable) ---
# create_tool_calling_agent æœ¬èº«è¿”å›çš„å°±æ˜¯ä¸€ä¸ª Runnableï¼Œä¸æ˜¯ Chain ç±»
agent_runnable = create_tool_calling_agent(llm, tools, get_chat_prompt())
# AgentExecutor æ˜¯ç›®å‰å”¯ä¸€çš„è¿è¡Œæ—¶å°è£… (LangGraph æ˜¯ä¸‹ä¸€ä»£æ›¿ä»£å“ï¼Œä½† Week 2 æš‚ä¸å¼•å…¥)
agent_executor = AgentExecutor(agent=agent_runnable, tools=tools, verbose=True)

# --- 3. æ„å»ºè®°å¿†æå–ç®¡é“ (Pure LCEL) ---
# è·å– prompt å’Œ parser
extract_prompt, extract_parser = get_extraction_components()

# ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šä½¿ç”¨ç®¡é“ç¬¦ | æ„å»º Runnable
# Input -> Prompt -> LLM -> Parser -> Structure Object
extraction_runnable = extract_prompt | llm | extract_parser

def run_chat_loop():
    print("ğŸ± èµ›åšå–µ (LCELç‰ˆ) å¯åŠ¨... (è¾“å…¥ 'exit' é€€å‡º)")
    chat_history = []
    
    while True:
        user_input = input("\nUser: ")
        if user_input.lower() == "exit": break
            
        # A. è¯»æ¡£
        current_profile_dict = profile_manager.load_profile()
        # è½¬æ¢æˆå­—ç¬¦ä¸²å–‚ç»™ Chat Agent
        profile_str = json.dumps(current_profile_dict, ensure_ascii=False)
        
        # B. èŠå¤© (Chat Loop)
        response = agent_executor.invoke({
            "input": user_input,
            "chat_history": chat_history,
            "user_profile": profile_str
        })
        ai_content = response["output"]
        print(f"CyberNeko: {ai_content}")
        
        # C. è®°å¿†æå– (Sidecar Extraction)
        # ç›´æ¥è°ƒç”¨ LCEL Runnableï¼Œå®ƒä¼šè‡ªåŠ¨è¿”å› UserProfile å¯¹è±¡ï¼Œæ— éœ€ json.loads
        try:
            print("\n[ç³»ç»Ÿåå°] æ­£åœ¨æå–è®°å¿† (LCEL Pipeline)...")
            extracted_profile: UserProfile = extraction_runnable.invoke({
                "input": user_input,
                "ai_response": ai_content
            })
            
            # D. å­˜æ¡£
            # åªæœ‰å½“æå–å‡ºçš„å¯¹è±¡é‡Œæœ‰å®è´¨å†…å®¹æ—¶æ‰æ›´æ–°
            if any(value for value in extracted_profile.model_dump().values()):
                new_data = profile_manager.update_profile(extracted_profile)
                print(f"[è®°å¿†æ›´æ–°] æˆåŠŸåˆå¹¶: {extracted_profile.model_dump(exclude_none=True)}")
            else:
                print("[è®°å¿†æ›´æ–°] æœªå‘ç°æ–°ä¿¡æ¯")
                
        except Exception as e:
            print(f"[ç³»ç»Ÿé”™è¯¯] {e}")

        # æ›´æ–°çŸ­æœŸå†å²
        chat_history.append(HumanMessage(content=user_input))
        chat_history.append(AIMessage(content=ai_content))

if __name__ == "__main__":
    run_chat_loop()